{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary functions that we will be using throughout the Jupyter sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we have implemented a function for every pre processing teqnique with a breif explanation into what it does\n",
    "# This approach was taken to keep our code more organinzed, and maintainable.\n",
    "# Every time we train our model and get our results, we can simply come back to the functions and tweak them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  return pd.read_csv(filename)\n",
    "\n",
    "def avg_trans_amt_per_card(data):\n",
    "    # Average Transaction Amount Per Card: Used to calculate the average amount of money \n",
    "    # spent in transactions for each credit card in the dataset. Once we have the average we \n",
    "    # will use it to compare each transaction amount to the average, and can help identify \n",
    "    # transactions that are too high.\n",
    "    # Group by credit card number and calculate the mean transaction amount\n",
    "    average_amt_per_card = data.groupby('cc_num')['amt'].mean().reset_index()\n",
    "    average_amt_per_card.rename(columns={'amt': 'avg_amt_per_card'}, inplace=True)\n",
    "    # Merge the average transaction amount per card back to both the train and test datasets\n",
    "    data = data.merge(average_amt_per_card, on='cc_num', how='left')\n",
    "    # Create a new column to compare transaction amount to the average per card\n",
    "    data['amt_vs_avg'] = data['amt'] / data['avg_amt_per_card']\n",
    "    return data\n",
    "\n",
    "def trans_freq_per_card(data):\n",
    "    # Transaction Frequency Per Card: Used to count how many transactions are made each \n",
    "    # day per credit card. Helps detect unusual activity if there are more transactions per \n",
    "    # day than the usual pattern.\n",
    "    # Calculate transaction frequency per card per day\n",
    "    trans_freq_per_card = data.groupby(['cc_num', 'trans_date']).size().reset_index(name='trans_freq_per_day')\n",
    "    data = data.merge(trans_freq_per_card, on=['cc_num', 'trans_date'], how='left')\n",
    "    return data\n",
    "\n",
    "def change_in_spending(data):\n",
    "    # Change in Spending Pattern Per Card: Used to compare the current transaction amount to the average\n",
    "    # amount spent for a similar category. By detecting significant deviations in spending patters per \n",
    "    # category we can detect fraud.\n",
    "    # Calculate average spending per card per category\n",
    "    avg_spending_per_card_category = data.groupby(['cc_num', 'category'])['amt'].transform('mean')\n",
    "    data['change_in_spending'] = data['amt'] / avg_spending_per_card_category\n",
    "    return data\n",
    "\n",
    "def handle_date_time(data):\n",
    "    # Converting trans_date_trans_time into a datetime object, then making new cols: trans_date and trans_time\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%m/%d/%y %H:%M') # converting the 'trans_date_trans_time' column from a string to a datetime object \n",
    "    data['trans_date'] = data['trans_date_trans_time'].dt.date # extracts the date part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_date'.\n",
    "    data['trans_time'] = data['trans_date_trans_time'].dt.time # extracts the time part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_time'.\n",
    "    return data # returns 2 new cols\n",
    "\n",
    "def dates_since_last_purchase(data):\n",
    "    # Days Since Last Purchase Per Card: Used to calculate the number of days between each transaction \n",
    "    # for the same credit card. Helps detect a pattern of how frequently the card is being used; for example, \n",
    "    # if the card is used 2-3 times a day and all of a sudden the card is being used 10 times a day for \n",
    "    # 2 days straight, it could be fraud.\n",
    "    # Ensure data is sorted by date for correct days calculation\n",
    "    data.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
    "    # Calculate the number of days between each transaction for the same credit card\n",
    "    data['days_since_last'] = data.groupby('cc_num')['trans_date_trans_time'].diff().dt.days.fillna(0).astype(int)\n",
    "    return data\n",
    "\n",
    "def convert_to_numerical_data(data):\n",
    "    # Convert 'trans_date_trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_date_trans_time'].dt.hour * 3600 + data['trans_date_trans_time'].dt.minute * 60 + data['trans_date_trans_time'].dt.second\n",
    "    \n",
    "    # Perform one-hot encoding for 'merchant' and 'category'\n",
    "    data = pd.get_dummies(data, columns=['merchant', 'category'])\n",
    "    \n",
    "    # Convert 'trans_date' to datetime and extract features\n",
    "    data['trans_date'] = pd.to_datetime(data['trans_date'])\n",
    "    data['trans_date_year'] = data['trans_date'].dt.year\n",
    "    data['trans_date_month'] = data['trans_date'].dt.month\n",
    "    data['trans_date_day'] = data['trans_date'].dt.day\n",
    "    # Convert 'trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
    "    # Drop the original non-numeric columns\n",
    "    data.drop(['trans_date_trans_time', 'trans_date', 'trans_time'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def drop_unnecessary_cols(data):\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'job', 'dob', 'trans_num', 'unix_time']\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "def separate_features_and_labels(data, label_column_name):\n",
    "    # Since 'is_fraud' is the label column, separate the features and labels\n",
    "    X = data.drop(label_column_name, axis=1)  # Drop the label column to create the features set\n",
    "    y = data[label_column_name]              # Get the label column as the labels set\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = handle_date_time(data)\n",
    "    data = avg_trans_amt_per_card(data)\n",
    "    data = trans_freq_per_card(data)\n",
    "    data = dates_since_last_purchase(data)\n",
    "    data = change_in_spending(data)\n",
    "    data = convert_to_numerical_data(data)\n",
    "    data = drop_unnecessary_cols(data)\n",
    "    # data = split_features(data)\n",
    "    return data\n",
    "\n",
    "def scale_data(X_train, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit the scaler only on the training data\n",
    "    scaler.fit(X_train)\n",
    "    # Apply the transformation to the training data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    # Apply the same transformation to the validation data\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------------- CONVERT NON NUMERICAL DATA ----------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Converting the non numerical data into numbers\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_numerical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_data \u001b[38;5;241m=\u001b[39m convert_to_numerical_data(test_data)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Drop unecessary cols\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 51\u001b[0m, in \u001b[0;36mconvert_to_numerical_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_numerical_data\u001b[39m(data):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Convert 'trans_date_trans_time' to total seconds elapsed since midnight\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_time_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrans_date_trans_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3600\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_date_trans_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mminute \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_date_trans_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39msecond\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Perform one-hot encoding for 'merchant' and 'category'\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerchant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Fraud Detect/venv/lib/python3.12/site-packages/pandas/core/generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   6297\u001b[0m ):\n\u001b[1;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Fraud Detect/venv/lib/python3.12/site-packages/pandas/core/accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/Desktop/Fraud Detect/venv/lib/python3.12/site-packages/pandas/core/indexes/accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# ------------------------------ LOAD DATA ------------------------------\n",
    "# Load data\n",
    "train_data = load_dataset('fraudTrain.csv')\n",
    "test_data = load_dataset('fraudTest.csv')\n",
    "\n",
    "# ------------------------------ DROP COLS ------------------------------\n",
    "# Remove first column since it is unnecessary\n",
    "train_data = train_data.iloc[:, 1:] # DONE\n",
    "test_data = test_data.iloc[:, 1:] # DONE\n",
    "\n",
    "# --------------------------- PREPROCESS DATA ---------------------------\n",
    "# Preporcess the data\n",
    "train_data = preprocess_data(train_data)\n",
    "test_data = preprocess_data(test_data)\n",
    "\n",
    "# --------------------------- SPLIT FEATURES ---------------------------\n",
    "X_train, y_train = separate_features_and_labels(train_data, 'is_fraud') # This is the only one were spliting into testing and training\n",
    "X_test, y_test = separate_features_and_labels(test_data, 'is_fraud')\n",
    "\n",
    "# -------------------------- PLAN FOR EMPTY VALS -------------------------\n",
    "# Ensure no missing values are left untreated\n",
    "# This method will fill any NaN values in your feature sets with the mean of \n",
    "#their respective columns, which can help in maintaining the integrity of your dataset \n",
    "# without dropping valuable data points.\n",
    "X_train.fillna(X_train.mean(), inplace=True)\n",
    "y_train.fillna(X_train.mean(), inplace=True)\n",
    "X_test.fillna(X_test.mean(), inplace=True)\n",
    "y_test.fillna(X_test.mean(), inplace=True)\n",
    "\n",
    "# print(X_train.columns)\n",
    "# print(X_test.columns)\n",
    "# print(\"Shape of X_train:\", X_train.shape)\n",
    "# print(\"Shape of y_train:\", y_train.shape)\n",
    "# print(\"Shape of X_train:\", X_test.shape)\n",
    "# print(\"Shape of y_train:\", y_test.shape)\n",
    "\n",
    "# Splitting the training data into training and validation sets:\n",
    "# train_test_split: This function is used to randomly split the \n",
    "# training data into new training data (X_train_final, y_train_final) \n",
    "# and validation data (X_val, y_val), ensuring that the validation \n",
    "# data is representative but not seen during training.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# X_train_scaled and X_val_scaled are ready for model training and validation\n",
    "X_train_scaled, X_val_scaled = scale_data(X_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.994005994005994\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       997\n",
      "           1       0.33      0.50      0.40         4\n",
      "\n",
      "    accuracy                           0.99      1001\n",
      "   macro avg       0.67      0.75      0.70      1001\n",
      "weighted avg       1.00      0.99      0.99      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- LOGISTIC REGRESSION MODEL ----------------------\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)  # Adjust max_iter if convergence issues occur\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "lr_val_predictions = lr_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_val, lr_val_predictions))\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_val, lr_val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with SMOTE Accuracy: 0.995004995004995\n",
      "Logistic Regression with SMOTE Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       997\n",
      "           1       0.43      0.75      0.55         4\n",
      "\n",
      "    accuracy                           1.00      1001\n",
      "   macro avg       0.71      0.87      0.77      1001\n",
      "weighted avg       1.00      1.00      1.00      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- LOGISTIC REGRESSION MODEL ----------------------\n",
    "# ------------------------------ USING SMOTE -----------------------------\n",
    "# Create a SMOTE object\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "\n",
    "# Create a pipeline that first oversamples and then runs the logistic regression\n",
    "pipeline = make_pipeline(smote, LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Fit on the training data\n",
    "pipeline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "lr_val_predictions = pipeline.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression with SMOTE Accuracy:\", accuracy_score(y_val, lr_val_predictions))\n",
    "print(\"Logistic Regression with SMOTE Classification Report:\")\n",
    "print(classification_report(y_val, lr_val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- LOGISTIC REGRESSION MODEL ----------------------\n",
    "# ------------------------------ USING SMOTE -----------------------------\n",
    "# Create a SMOTE object\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "\n",
    "# Create a pipeline that first oversamples and then runs the logistic regression\n",
    "pipeline = make_pipeline(smote, LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Fit on the training data\n",
    "pipeline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "lr_val_predictions = pipeline.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression with SMOTE Accuracy:\", accuracy_score(y_val, lr_val_predictions))\n",
    "print(\"Logistic Regression with SMOTE Classification Report:\")\n",
    "print(classification_report(y_val, lr_val_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
