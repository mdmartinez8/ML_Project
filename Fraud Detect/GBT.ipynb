{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b165588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb1c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  return pd.read_csv(filename)\n",
    "\n",
    "def avg_trans_amt_per_card(data):\n",
    "    # Average Transaction Amount Per Card: Used to calculate the average amount of money \n",
    "    # spent in transactions for each credit card in the dataset. Once we have the average we \n",
    "    # will use it to compare each transaction amount to the average, and can help identify \n",
    "    # transactions that are too high.\n",
    "    # Group by credit card number and calculate the mean transaction amount\n",
    "    average_amt_per_card = data.groupby('cc_num')['amt'].mean().reset_index()\n",
    "    average_amt_per_card.rename(columns={'amt': 'avg_amt_per_card'}, inplace=True)\n",
    "    # Merge the average transaction amount per card back to both the train and test datasets\n",
    "    data = data.merge(average_amt_per_card, on='cc_num', how='left')\n",
    "    # Create a new column to compare transaction amount to the average per card\n",
    "    data['amt_vs_avg'] = data['amt'] / data['avg_amt_per_card']\n",
    "    return data\n",
    "\n",
    "def trans_freq_per_card(data):\n",
    "    # Transaction Frequency Per Card: Used to count how many transactions are made each \n",
    "    # day per credit card. Helps detect unusual activity if there are more transactions per \n",
    "    # day than the usual pattern.\n",
    "    # Calculate transaction frequency per card per day\n",
    "    trans_freq_per_card = data.groupby(['cc_num', 'trans_date']).size().reset_index(name='trans_freq_per_day')\n",
    "    data = data.merge(trans_freq_per_card, on=['cc_num', 'trans_date'], how='left')\n",
    "    return data\n",
    "\n",
    "def change_in_spending(data):\n",
    "    # Change in Spending Pattern Per Card: Used to compare the current transaction amount to the average\n",
    "    # amount spent for a similar category. By detecting significant deviations in spending patters per \n",
    "    # category we can detect fraud.\n",
    "    # Calculate average spending per card per category\n",
    "    avg_spending_per_card_category = data.groupby(['cc_num', 'category'])['amt'].transform('mean')\n",
    "    data['change_in_spending'] = data['amt'] / avg_spending_per_card_category\n",
    "    return data\n",
    "\n",
    "def handle_date_time(data):\n",
    "    # Converting trans_date_trans_time into a datetime object, then making new cols: trans_date and trans_time\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%m/%d/%y %H:%M') # converting the 'trans_date_trans_time' column from a string to a datetime object \n",
    "    data['trans_date'] = data['trans_date_trans_time'].dt.date # extracts the date part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_date'.\n",
    "    data['trans_time'] = data['trans_date_trans_time'].dt.time # extracts the time part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_time'.\n",
    "    return data # returns 2 new cols\n",
    "\n",
    "def dates_since_last_purchase(data):\n",
    "    # Days Since Last Purchase Per Card: Used to calculate the number of days between each transaction \n",
    "    # for the same credit card. Helps detect a pattern of how frequently the card is being used; for example, \n",
    "    # if the card is used 2-3 times a day and all of a sudden the card is being used 10 times a day for \n",
    "    # 2 days straight, it could be fraud.\n",
    "    # Ensure data is sorted by date for correct days calculation\n",
    "    data.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
    "    # Calculate the number of days between each transaction for the same credit card\n",
    "    data['days_since_last'] = data.groupby('cc_num')['trans_date_trans_time'].diff().dt.days.fillna(0).astype(int)\n",
    "    return data\n",
    "\n",
    "def convert_to_numerical_data(data):\n",
    "    # Convert 'trans_date_trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_date_trans_time'].dt.hour * 3600 + data['trans_date_trans_time'].dt.minute * 60 + data['trans_date_trans_time'].dt.second\n",
    "    \n",
    "    # Perform one-hot encoding for 'merchant' and 'category'\n",
    "    data = pd.get_dummies(data, columns=['merchant', 'category'])\n",
    "    \n",
    "    # Convert 'trans_date' to datetime and extract features\n",
    "    data['trans_date'] = pd.to_datetime(data['trans_date'])\n",
    "    data['trans_date_year'] = data['trans_date'].dt.year\n",
    "    data['trans_date_month'] = data['trans_date'].dt.month\n",
    "    data['trans_date_day'] = data['trans_date'].dt.day\n",
    "    # Convert 'trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
    "    # Drop the original non-numeric columns\n",
    "    data.drop(['trans_date_trans_time', 'trans_date', 'trans_time'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def drop_unnecessary_cols(data):\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'job', 'dob', 'trans_num', 'unix_time']\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "def separate_features_and_labels(data, label_column_name):\n",
    "    # Since 'is_fraud' is the label column, separate the features and labels\n",
    "    X = data.drop(label_column_name, axis=1)  # Drop the label column to create the features set\n",
    "    y = data[label_column_name]              # Get the label column as the labels set\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = handle_date_time(data)\n",
    "    data = avg_trans_amt_per_card(data)\n",
    "    data = trans_freq_per_card(data)\n",
    "    data = dates_since_last_purchase(data)\n",
    "    data = change_in_spending(data)\n",
    "    data = convert_to_numerical_data(data)\n",
    "    data = drop_unnecessary_cols(data)\n",
    "    # data = split_features(data)\n",
    "    return data\n",
    "\n",
    "def scale_data(X_train, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit the scaler only on the training data\n",
    "    scaler.fit(X_train)\n",
    "    # Apply the transformation to the training data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    # Apply the same transformation to the validation data\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154d6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('fraudTrain.csv')\n",
    "Y = pd.read_csv('fraudTest.csv')\n",
    "# Prep the data\n",
    "# Separate features and target\n",
    "Y = X['is_fraud']  # assuming 'is_fraud' is the target column\n",
    "X = X.drop(columns=['is_fraud'])\n",
    "X = preprocess_data(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# categorical_columns = ['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'trans_date_trans_time', 'dob', 'trans_num']\n",
    "# # Performs One-Hot-Encoding on string data\n",
    "# X_train = pd.get_dummies(X_train, columns=categorical_columns)\n",
    "\n",
    "# X_test = pd.get_dummies(X_test, columns=categorical_columns)\n",
    "\n",
    "# Y_train = pd.get_dummies(Y_train, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7bbb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0        cc_num     amt      lat      long  city_pop  \\\n",
      "1847        1847  4.358138e+18    2.93  42.7012  -92.0762        53   \n",
      "4826        4826  3.013520e+13   78.03  39.1412  -94.3515    123373   \n",
      "4761        4761  4.587577e+15    3.76  44.0943  -69.4828      1643   \n",
      "3977        3977  6.011367e+15   66.73  40.5046  -77.7186      4653   \n",
      "645          645  3.576432e+15   66.41  33.5623 -112.0559   1312922   \n",
      "...          ...           ...     ...      ...       ...       ...   \n",
      "1181        1181  6.526777e+15   26.07  40.6868  -73.8230     47211   \n",
      "2561        2561  4.294930e+12    1.42  37.3696  -80.1284      1363   \n",
      "3503        3503  3.590737e+15  125.39  34.5091  -92.4828      4074   \n",
      "1340        1340  4.681601e+15   30.94  39.9148  -80.7310     16183   \n",
      "2929        2929  3.023050e+13  515.33  40.9661  -76.8575       645   \n",
      "\n",
      "      merch_lat  merch_long  avg_amt_per_card  amt_vs_avg  ...  \\\n",
      "1847  43.080413  -91.680917         33.585000    0.087241  ...   \n",
      "4826  40.107979  -94.736899         49.822500    1.566160  ...   \n",
      "4761  44.949120  -70.329586         45.732857    0.082217  ...   \n",
      "3977  40.427864  -77.748825         33.920000    1.967276  ...   \n",
      "645   33.303045 -111.642781         69.973500    0.949074  ...   \n",
      "...         ...         ...               ...         ...  ...   \n",
      "1181  41.664070  -74.126579         40.442500    0.644619  ...   \n",
      "2561  36.930576  -79.878017         18.773333    0.075639  ...   \n",
      "3503  34.642336  -93.429751         96.843636    1.294768  ...   \n",
      "1340  40.787864  -80.169766         80.548571    0.384116  ...   \n",
      "2929  40.654686  -75.993437        155.906000    3.305389  ...   \n",
      "\n",
      "      category_kids_pets  category_misc_net  category_misc_pos  \\\n",
      "1847               False              False              False   \n",
      "4826               False              False              False   \n",
      "4761               False              False              False   \n",
      "3977               False              False              False   \n",
      "645                False              False              False   \n",
      "...                  ...                ...                ...   \n",
      "1181               False              False              False   \n",
      "2561               False              False              False   \n",
      "3503               False              False              False   \n",
      "1340               False              False              False   \n",
      "2929               False              False              False   \n",
      "\n",
      "      category_personal_care  category_shopping_net  category_shopping_pos  \\\n",
      "1847                   False                  False                  False   \n",
      "4826                   False                  False                  False   \n",
      "4761                   False                  False                  False   \n",
      "3977                   False                  False                  False   \n",
      "645                    False                  False                  False   \n",
      "...                      ...                    ...                    ...   \n",
      "1181                   False                  False                  False   \n",
      "2561                   False                   True                  False   \n",
      "3503                   False                  False                  False   \n",
      "1340                   False                  False                  False   \n",
      "2929                   False                  False                  False   \n",
      "\n",
      "      category_travel  trans_date_year  trans_date_month  trans_date_day  \n",
      "1847            False             2019                 1               1  \n",
      "4826            False             2019                 1               4  \n",
      "4761            False             2019                 1               3  \n",
      "3977            False             2019                 1               3  \n",
      "645             False             2019                 1               1  \n",
      "...               ...              ...               ...             ...  \n",
      "1181            False             2019                 1               1  \n",
      "2561            False             2019                 1               2  \n",
      "3503            False             2019                 1               2  \n",
      "1340            False             2019                 1               1  \n",
      "2929             True             2019                 1               2  \n",
      "\n",
      "[4000 rows x 723 columns]\n",
      "Classification Accuracy: 0.996003996003996\n",
      " Classification Mean Squared Error: 0.003529617916169468\n",
      "Regression Mean Squared Error: 0.003529617916169468\n"
     ]
    }
   ],
   "source": [
    "# Initialize Models\n",
    "# For classification\n",
    "modelC = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# For regression\n",
    "modelR = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "print(X_train)\n",
    "# Train models\n",
    "modelC.fit(X_train, Y_train)\n",
    "modelR.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_R = modelR.predict(X_test)\n",
    "predictions_C = modelC.predict(X_test)\n",
    "\n",
    "# For classification\n",
    "accuracy = accuracy_score(Y_test, predictions_C)\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "mse = mean_squared_error(Y_test, predictions_R)\n",
    "print(f\" Classification Mean Squared Error: {mse}\")\n",
    "\n",
    "# For regression\n",
    "mse = mean_squared_error(Y_test, predictions_R)\n",
    "print(f\"Regression Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036f9b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "new_predictions = model.predict(new_data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
