{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b165588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb1c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  return pd.read_csv(filename)\n",
    "\n",
    "def avg_trans_amt_per_card(data):\n",
    "    # Average Transaction Amount Per Card: Used to calculate the average amount of money \n",
    "    # spent in transactions for each credit card in the dataset. Once we have the average we \n",
    "    # will use it to compare each transaction amount to the average, and can help identify \n",
    "    # transactions that are too high.\n",
    "    # Group by credit card number and calculate the mean transaction amount\n",
    "    average_amt_per_card = data.groupby('cc_num')['amt'].mean().reset_index()\n",
    "    average_amt_per_card.rename(columns={'amt': 'avg_amt_per_card'}, inplace=True)\n",
    "    # Merge the average transaction amount per card back to both the train and test datasets\n",
    "    data = data.merge(average_amt_per_card, on='cc_num', how='left')\n",
    "    # Create a new column to compare transaction amount to the average per card\n",
    "    data['amt_vs_avg'] = data['amt'] / data['avg_amt_per_card']\n",
    "    return data\n",
    "\n",
    "def trans_freq_per_card(data):\n",
    "    # Transaction Frequency Per Card: Used to count how many transactions are made each \n",
    "    # day per credit card. Helps detect unusual activity if there are more transactions per \n",
    "    # day than the usual pattern.\n",
    "    # Calculate transaction frequency per card per day\n",
    "    trans_freq_per_card = data.groupby(['cc_num', 'trans_date']).size().reset_index(name='trans_freq_per_day')\n",
    "    data = data.merge(trans_freq_per_card, on=['cc_num', 'trans_date'], how='left')\n",
    "    return data\n",
    "\n",
    "def change_in_spending(data):\n",
    "    # Change in Spending Pattern Per Card: Used to compare the current transaction amount to the average\n",
    "    # amount spent for a similar category. By detecting significant deviations in spending patters per \n",
    "    # category we can detect fraud.\n",
    "    # Calculate average spending per card per category\n",
    "    avg_spending_per_card_category = data.groupby(['cc_num', 'category'])['amt'].transform('mean')\n",
    "    data['change_in_spending'] = data['amt'] / avg_spending_per_card_category\n",
    "    return data\n",
    "\n",
    "def handle_date_time(data):\n",
    "    # Converting trans_date_trans_time into a datetime object, then making new cols: trans_date and trans_time\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%m/%d/%y %H:%M') # converting the 'trans_date_trans_time' column from a string to a datetime object \n",
    "    data['trans_date'] = data['trans_date_trans_time'].dt.date # extracts the date part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_date'.\n",
    "    data['trans_time'] = data['trans_date_trans_time'].dt.time # extracts the time part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_time'.\n",
    "    return data # returns 2 new cols\n",
    "\n",
    "def dates_since_last_purchase(data):\n",
    "    # Days Since Last Purchase Per Card: Used to calculate the number of days between each transaction \n",
    "    # for the same credit card. Helps detect a pattern of how frequently the card is being used; for example, \n",
    "    # if the card is used 2-3 times a day and all of a sudden the card is being used 10 times a day for \n",
    "    # 2 days straight, it could be fraud.\n",
    "    # Ensure data is sorted by date for correct days calculation\n",
    "    data.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
    "    # Calculate the number of days between each transaction for the same credit card\n",
    "    data['days_since_last'] = data.groupby('cc_num')['trans_date_trans_time'].diff().dt.days.fillna(0).astype(int)\n",
    "    return data\n",
    "\n",
    "def convert_to_numerical_data(data):\n",
    "    # Convert 'trans_date_trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_date_trans_time'].dt.hour * 3600 + data['trans_date_trans_time'].dt.minute * 60 + data['trans_date_trans_time'].dt.second\n",
    "    \n",
    "    # Perform one-hot encoding for 'merchant' and 'category'\n",
    "    data = pd.get_dummies(data, columns=['merchant', 'category'])\n",
    "    \n",
    "    # Convert 'trans_date' to datetime and extract features\n",
    "    data['trans_date'] = pd.to_datetime(data['trans_date'])\n",
    "    data['trans_date_year'] = data['trans_date'].dt.year\n",
    "    data['trans_date_month'] = data['trans_date'].dt.month\n",
    "    data['trans_date_day'] = data['trans_date'].dt.day\n",
    "    # Convert 'trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
    "    # Drop the original non-numeric columns\n",
    "    data.drop(['trans_date_trans_time', 'trans_date', 'trans_time'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def drop_unnecessary_cols(data):\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'job', 'dob', 'trans_num', 'unix_time']\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "def separate_features_and_labels(data, label_column_name):\n",
    "    # Since 'is_fraud' is the label column, separate the features and labels\n",
    "    X = data.drop(label_column_name, axis=1)  # Drop the label column to create the features set\n",
    "    y = data[label_column_name]              # Get the label column as the labels set\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = handle_date_time(data)\n",
    "    data = avg_trans_amt_per_card(data)\n",
    "    data = trans_freq_per_card(data)\n",
    "    data = dates_since_last_purchase(data)\n",
    "    data = change_in_spending(data)\n",
    "    data = convert_to_numerical_data(data)\n",
    "    data = drop_unnecessary_cols(data)\n",
    "    # data = split_features(data)\n",
    "    return data\n",
    "\n",
    "def scale_data(X_train, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit the scaler only on the training data\n",
    "    scaler.fit(X_train)\n",
    "    # Apply the transformation to the training data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    # Apply the same transformation to the validation data\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154d6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('fraudTrain.csv')\n",
    "Y = pd.read_csv('fraudTest.csv')\n",
    "# Prep the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate features and target\n",
    "Y = X['is_fraud']  # assuming 'is_fraud' is the target column\n",
    "X = X.drop(columns=['is_fraud'])\n",
    "X = preprocess_data(X)\n",
    "\n",
    "categorical_columns = ['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'trans_date_trans_time', 'dob', 'trans_num']\n",
    "# Performs One-Hot-Encoding on string data\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_columns)\n",
    "\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7bbb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0 trans_date_trans_time        cc_num  \\\n",
      "4677        4677         6/22/20 17:09  4.292744e+18   \n",
      "800          800         6/21/20 16:48  3.560698e+15   \n",
      "3671        3671         6/22/20 12:21  3.531130e+15   \n",
      "4193        4193         6/22/20 14:48  6.011367e+15   \n",
      "2968        2968          6/22/20 7:27  3.772340e+14   \n",
      "...          ...                   ...           ...   \n",
      "4426        4426         6/22/20 15:54  3.764450e+14   \n",
      "466          466         6/21/20 14:55  4.951647e+15   \n",
      "3092        3092          6/22/20 8:20  4.365383e+15   \n",
      "3772        3772         6/22/20 12:54  2.288814e+15   \n",
      "860          860         6/21/20 17:09  4.198470e+12   \n",
      "\n",
      "                                merchant       category     amt     first  \\\n",
      "4677             fraud_Bahringer-Streich    food_dining   69.70   Michael   \n",
      "800               fraud_Pacocha-Weissnat      kids_pets   47.28   Matthew   \n",
      "3671                  fraud_Kihn-Fritsch    food_dining   18.38    Shelby   \n",
      "4193       fraud_Roob, Conn and Tremblay   shopping_pos  243.23      Adam   \n",
      "2968      fraud_Moore, Dibbert and Koepp       misc_net    9.46   Theresa   \n",
      "...                                  ...            ...     ...       ...   \n",
      "4426              fraud_Pfeffer and Sons   shopping_pos    3.81    Rachel   \n",
      "466                     fraud_Skiles LLC           home   45.67  Kimberly   \n",
      "3092                     fraud_Price Inc   shopping_net   97.81    Robert   \n",
      "3772  fraud_Schroeder, Hauck and Treutel  entertainment   28.98   Barbara   \n",
      "860   fraud_Streich, Dietrich and Barton   shopping_net    3.58  Christie   \n",
      "\n",
      "            last gender                         street  ...      lat  \\\n",
      "4677    Williams      M  35822 Clayton Street Apt. 679  ...  38.2674   \n",
      "800        Young      M     8840 Miller Port Suite 645  ...  33.7163   \n",
      "3671    Mitchell      F               974 Cindy Stream  ...  43.8065   \n",
      "4193       Stark      M      0912 Mark Fields Apt. 080  ...  40.5046   \n",
      "2968   Blackwell      F         43576 Kristina Islands  ...  39.3716   \n",
      "...          ...    ...                            ...  ...      ...   \n",
      "4426        Lowe      F             372 Jeffrey Course  ...  41.1558   \n",
      "466       Miller      F           75533 Tamara Valleys  ...  37.9943   \n",
      "3092     Goodman      M      956 Paul Fields Suite 108  ...  48.1439   \n",
      "3772      Norman      F          6278 Stephanie Unions  ...  40.8265   \n",
      "860   Williamson      F                519 Jerry Views  ...  41.4768   \n",
      "\n",
      "          long  city_pop                                      job       dob  \\\n",
      "4677  -76.4954      5927                            Art therapist    6/9/73   \n",
      "800  -116.3381      4677                          Learning mentor    5/6/55   \n",
      "3671  -73.0882      5895                        Scientist, marine   7/13/75   \n",
      "4193  -77.7186      4653                    Nutritional therapist    7/1/97   \n",
      "2968  -77.8229      1925                        Systems developer   2/14/66   \n",
      "...        ...       ...                                      ...       ...   \n",
      "4426 -101.1360      1789                         Insurance broker   2/11/82   \n",
      "466   -88.9417       324  Scientist, research (physical sciences)   6/15/76   \n",
      "3092  -92.8561      1680                  Horticultural therapist  12/10/76   \n",
      "3772  -73.9383   1577385                                Herbalist   8/29/81   \n",
      "860   -95.3509      2036                    Engineering geologist   8/20/71   \n",
      "\n",
      "                             trans_num   unix_time  merch_lat  merch_long  \\\n",
      "4677  37e60fce92ee7e04e2d429a7cd3962a0  1371920961  38.735038  -77.401301   \n",
      "800   28c6fd35e4b576c1d587df72a938b7ae  1371833306  34.543180 -115.448531   \n",
      "3671  99fb64f8714b828c7a74e991174dffa3  1371903715  44.087314  -73.464019   \n",
      "4193  a0619abfacbe2688d00080dde2e7c8e0  1371912525  39.683505  -78.705316   \n",
      "2968  7792f7618779145802b01a2de89fcb8d  1371886039  39.765552  -77.747955   \n",
      "...                                ...         ...        ...         ...   \n",
      "4426  18677633fcfb0d9ee77fdf37d154fe49  1371916465  40.986227 -101.875263   \n",
      "466   62b1c65b11e31f740ae17771d2646965  1371826517  37.122514  -88.328393   \n",
      "3092  f395fc05edb7d69f37dec91f0cd7185d  1371889235  47.286472  -93.212163   \n",
      "3772  de97c9bce92908c5f65638fec8836f2b  1371905662  39.987808  -73.016473   \n",
      "860   e76fed346feb28196ceeefc92c508f7b  1371834548  40.486739  -95.008421   \n",
      "\n",
      "      is_fraud  \n",
      "4677         0  \n",
      "800          0  \n",
      "3671         0  \n",
      "4193         0  \n",
      "2968         0  \n",
      "...        ...  \n",
      "4426         0  \n",
      "466          0  \n",
      "3092         0  \n",
      "3772         0  \n",
      "860          0  \n",
      "\n",
      "[4000 rows x 23 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_train)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodelC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m modelR\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:665\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    663\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight_is_none:\n\u001b[0;32m--> 665\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_y(y\u001b[38;5;241m=\u001b[39my, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:1500\u001b[0m, in \u001b[0;36mGradientBoostingClassifier._encode_y\u001b[0;34m(self, y, sample_weight)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, sample_weight):\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;66;03m# encode classes into 0 ... n_classes - 1 and sets attributes classes_\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;66;03m# and n_trees_per_iteration_\u001b[39;00m\n\u001b[0;32m-> 1500\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m     label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m   1503\u001b[0m     encoded_y_int \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/multiclass.py:213\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_classification_targets\u001b[39m(y):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that target y is of a non-regression type.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    Only the following target types (as defined in type_of_target) are allowed:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m        Target values.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m     ]:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/multiclass.py:316\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/multiclass.py:193\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (labels\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (labels\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels))\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_integral_float(labels))  \u001b[38;5;66;03m# bool, int, uint\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    196\u001b[0m         xp\u001b[38;5;241m.\u001b[39misdtype(y\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigned integer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsigned integer\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _is_integral_float(labels)\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_array_api.py:307\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# Initialize Models\n",
    "# For classification\n",
    "modelC = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# For regression\n",
    "modelR = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "print(Y_train)\n",
    "# Train models\n",
    "modelC.fit(X_train, Y_train)\n",
    "modelR.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_R = modelR.predict(X_test)\n",
    "predictions_C = modelC.predict(X_test)\n",
    "\n",
    "# For classification\n",
    "accuracy = accuracy_score(Y_test, predictions_C)\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "mse = mean_squared_error(Y_test, predictions_R)\n",
    "print(f\" Classification Mean Squared Error: {mse}\")\n",
    "\n",
    "# For regression\n",
    "mse = mean_squared_error(Y_test, predictions_R)\n",
    "print(f\"Regression Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036f9b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "new_predictions = model.predict(new_data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
