{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "def load_dataset(filename):\n",
    "  return pd.read_csv(filename)\n",
    "\n",
    "def avg_trans_amt_per_card(data):\n",
    "    # Average Transaction Amount Per Card: Used to calculate the average amount of money \n",
    "    # spent in transactions for each credit card in the dataset. Once we have the average we \n",
    "    # will use it to compare each transaction amount to the average, and can help identify \n",
    "    # transactions that are too high.\n",
    "    # Group by credit card number and calculate the mean transaction amount\n",
    "    average_amt_per_card = data.groupby('cc_num')['amt'].mean().reset_index()\n",
    "    average_amt_per_card.rename(columns={'amt': 'avg_amt_per_card'}, inplace=True)\n",
    "    # Merge the average transaction amount per card back to both the train and test datasets\n",
    "    data = data.merge(average_amt_per_card, on='cc_num', how='left')\n",
    "    # Create a new column to compare transaction amount to the average per card\n",
    "    data['amt_vs_avg'] = data['amt'] / data['avg_amt_per_card']\n",
    "    return data\n",
    "\n",
    "def trans_freq_per_card(data):\n",
    "    # Transaction Frequency Per Card: Used to count how many transactions are made each \n",
    "    # day per credit card. Helps detect unusual activity if there are more transactions per \n",
    "    # day than the usual pattern.\n",
    "    # Calculate transaction frequency per card per day\n",
    "    trans_freq_per_card = data.groupby(['cc_num', 'trans_date']).size().reset_index(name='trans_freq_per_day')\n",
    "    data = data.merge(trans_freq_per_card, on=['cc_num', 'trans_date'], how='left')\n",
    "    return data\n",
    "\n",
    "def change_in_spending(data):\n",
    "    # Change in Spending Pattern Per Card: Used to compare the current transaction amount to the average\n",
    "    # amount spent for a similar category. By detecting significant deviations in spending patters per \n",
    "    # category we can detect fraud.\n",
    "    # Calculate average spending per card per category\n",
    "    avg_spending_per_card_category = data.groupby(['cc_num', 'category'])['amt'].transform('mean')\n",
    "    data['change_in_spending'] = data['amt'] / avg_spending_per_card_category\n",
    "    return data\n",
    "\n",
    "def handle_date_time(data):\n",
    "    # Converting trans_date_trans_time into a datetime object, then making new cols: trans_date and trans_time\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%m/%d/%y %H:%M') # converting the 'trans_date_trans_time' column from a string to a datetime object \n",
    "    data['trans_date'] = data['trans_date_trans_time'].dt.date # extracts the date part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_date'.\n",
    "    data['trans_time'] = data['trans_date_trans_time'].dt.time # extracts the time part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_time'.\n",
    "    return data # returns 2 new cols\n",
    "\n",
    "def dates_since_last_purchase(data):\n",
    "    # Days Since Last Purchase Per Card: Used to calculate the number of days between each transaction \n",
    "    # for the same credit card. Helps detect a pattern of how frequently the card is being used; for example, \n",
    "    # if the card is used 2-3 times a day and all of a sudden the card is being used 10 times a day for \n",
    "    # 2 days straight, it could be fraud.\n",
    "    # Ensure data is sorted by date for correct days calculation\n",
    "    data.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
    "    # Calculate the number of days between each transaction for the same credit card\n",
    "    data['days_since_last'] = data.groupby('cc_num')['trans_date_trans_time'].diff().dt.days.fillna(0).astype(int)\n",
    "    return data\n",
    "\n",
    "def convert_to_numerical_data(data):\n",
    "    # Convert 'trans_date_trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_date_trans_time'].dt.hour * 3600 + data['trans_date_trans_time'].dt.minute * 60 + data['trans_date_trans_time'].dt.second\n",
    "    \n",
    "    # Perform one-hot encoding for 'merchant' and 'category'\n",
    "    data = pd.get_dummies(data, columns=['merchant', 'category'])\n",
    "    \n",
    "    # Convert 'trans_date' to datetime and extract features\n",
    "    data['trans_date'] = pd.to_datetime(data['trans_date'])\n",
    "    data['trans_date_year'] = data['trans_date'].dt.year\n",
    "    data['trans_date_month'] = data['trans_date'].dt.month\n",
    "    data['trans_date_day'] = data['trans_date'].dt.day\n",
    "    # Convert 'trans_time' to total seconds elapsed since midnight\n",
    "    data['trans_time_seconds'] = data['trans_time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
    "    # Drop the original non-numeric columns\n",
    "    data.drop(['trans_date_trans_time', 'trans_date', 'trans_time'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def drop_unnecessary_cols(data):\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'job', 'dob', 'trans_num', 'unix_time']\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "def separate_features_and_labels(data, label_column_name):\n",
    "    # Since 'is_fraud' is the label column, separate the features and labels\n",
    "    X = data.drop(label_column_name, axis=1)  # Drop the label column to create the features set\n",
    "    y = data[label_column_name]              # Get the label column as the labels set\n",
    "    return X, y\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = handle_date_time(data)\n",
    "    data = avg_trans_amt_per_card(data)\n",
    "    data = trans_freq_per_card(data)\n",
    "    data = dates_since_last_purchase(data)\n",
    "    data = change_in_spending(data)\n",
    "    data = convert_to_numerical_data(data)\n",
    "    data = drop_unnecessary_cols(data)\n",
    "    # data = split_features(data)\n",
    "    return data\n",
    "\n",
    "def scale_data(X_train, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit the scaler only on the training data\n",
    "    scaler.fit(X_train)\n",
    "    # Apply the transformation to the training data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    # Apply the same transformation to the validation data\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ LOAD DATA ------------------------------\n",
    "# Load data\n",
    "train_data = load_dataset('fraudTrain.csv')\n",
    "test_data = load_dataset('fraudTest.csv')\n",
    "\n",
    "# ------------------------------ DROP COLS ------------------------------\n",
    "# Remove first column since it is unnecessary\n",
    "train_data = train_data.iloc[:, 1:] # DONE\n",
    "test_data = test_data.iloc[:, 1:] # DONE\n",
    "\n",
    "# --------------------------- PREPROCESS DATA ---------------------------\n",
    "# Preporcess the data\n",
    "train_data = preprocess_data(train_data)\n",
    "test_data = preprocess_data(test_data)\n",
    "\n",
    "# --------------------------- SPLIT FEATURES ---------------------------\n",
    "X_train, y_train = separate_features_and_labels(train_data, 'is_fraud') # This is the only one were spliting into testing and training\n",
    "X_test, y_test = separate_features_and_labels(test_data, 'is_fraud')\n",
    "# Prep the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# # Separate features and target\n",
    "# Y = X['is_fraud']  # assuming 'is_fraud' is the target column\n",
    "# X = X.drop(columns=['is_fraud'])\n",
    "# X = preprocess_data(X)\n",
    "\n",
    "# categorical_columns = ['merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'trans_date_trans_time', 'dob', 'trans_num']\n",
    "# # Performs One-Hot-Encoding on string data\n",
    "# X_train = pd.get_dummies(X_train, columns=categorical_columns)\n",
    "\n",
    "# X_test = pd.get_dummies(X_test, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cc_num     amt      lat      long  city_pop  merch_lat  \\\n",
      "1772  6.011368e+15    1.68  41.2419  -81.7453      7646  41.490778   \n",
      "986   4.561550e+12    3.27  34.9889 -106.0609      7268  35.863489   \n",
      "609   3.819900e+13  127.20  43.1960  -72.3001       477  43.324280   \n",
      "2912  2.242543e+15   55.70  38.4921  -85.4524       564  37.645404   \n",
      "791   4.661996e+18  105.68  40.8555  -79.7372      2054  41.776480   \n",
      "...            ...     ...      ...       ...       ...        ...   \n",
      "3396  3.576432e+15   32.61  33.5623 -112.0559   1312922  32.972587   \n",
      "4589  2.242543e+15   12.59  38.4921  -85.4524       564  39.315427   \n",
      "3629  4.425161e+15  100.69  31.4647 -100.3900    103927  31.969096   \n",
      "72    3.027300e+13   57.23  35.5762  -91.4539       111  36.144895   \n",
      "61    3.547560e+15   56.66  44.5232  -86.2061       680  44.005290   \n",
      "\n",
      "      merch_long  avg_amt_per_card  amt_vs_avg  trans_freq_per_day  ...  \\\n",
      "1772  -82.295114         72.142000    0.023287                   5  ...   \n",
      "986  -105.606809         52.201667    0.062642                   5  ...   \n",
      "609   -73.067471         93.516667    1.360185                   4  ...   \n",
      "2912  -85.898939         48.198750    1.155632                   2  ...   \n",
      "791   -79.918329        102.848000    1.027536                   2  ...   \n",
      "...          ...               ...         ...                 ...  ...   \n",
      "3396 -111.923682         69.973500    0.466034                   7  ...   \n",
      "4589  -86.151969         48.198750    0.261210                   3  ...   \n",
      "3629 -100.327230         48.930000    2.057838                   2  ...   \n",
      "72    -92.188172         67.903750    0.842811                   3  ...   \n",
      "61    -86.251990         31.762222    1.783880                   6  ...   \n",
      "\n",
      "      category_kids_pets  category_misc_net  category_misc_pos  \\\n",
      "1772               False              False              False   \n",
      "986                False              False              False   \n",
      "609                False              False              False   \n",
      "2912               False              False              False   \n",
      "791                False              False              False   \n",
      "...                  ...                ...                ...   \n",
      "3396               False              False              False   \n",
      "4589               False              False              False   \n",
      "3629               False              False              False   \n",
      "72                 False              False              False   \n",
      "61                 False              False              False   \n",
      "\n",
      "      category_personal_care  category_shopping_net  category_shopping_pos  \\\n",
      "1772                    True                  False                  False   \n",
      "986                    False                   True                  False   \n",
      "609                    False                  False                   True   \n",
      "2912                   False                  False                  False   \n",
      "791                    False                  False                  False   \n",
      "...                      ...                    ...                    ...   \n",
      "3396                   False                  False                   True   \n",
      "4589                   False                  False                  False   \n",
      "3629                   False                  False                  False   \n",
      "72                     False                  False                  False   \n",
      "61                     False                  False                  False   \n",
      "\n",
      "      category_travel  trans_date_year  trans_date_month  trans_date_day  \n",
      "1772            False             2019                 1               1  \n",
      "986             False             2019                 1               1  \n",
      "609             False             2019                 1               1  \n",
      "2912            False             2019                 1               2  \n",
      "791             False             2019                 1               1  \n",
      "...               ...              ...               ...             ...  \n",
      "3396            False             2019                 1               2  \n",
      "4589            False             2019                 1               3  \n",
      "3629            False             2019                 1               3  \n",
      "72              False             2019                 1               1  \n",
      "61              False             2019                 1               1  \n",
      "\n",
      "[1001 rows x 722 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "# Create DMatrices, which is XGBoost's optimized data structure\n",
    "dtrain = xgb.DMatrix(X_train, label=Y_train, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test, label=Y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.1,      # the training step for each iteration\n",
    "    'objective': 'binary:logistic',  # binary classification \n",
    "    'eval_metric': 'logloss'  # evaluation metric\n",
    "}\n",
    "num_rounds = 100  # the number of training iterations\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "preds = model.predict(dtest)\n",
    "preds = (preds > 0.5).astype('int')  # Convert probabilities to 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 1001\n",
      "Accuracy: 99.90%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       997\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           1.00      1001\n",
      "   macro avg       1.00      0.88      0.93      1001\n",
      "weighted avg       1.00      1.00      1.00      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Accuracy\n",
    "print(len(preds), len(Y_test))\n",
    "accuracy = accuracy_score(Y_test, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "\n",
    "report = classification_report(Y_test, preds)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
