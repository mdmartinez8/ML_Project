{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ALp4TL9J4ph"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary functions that we will be using throughout the Jupyter sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cHGrqPrBJ4pi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0J1Ot4ioJ4pk"
      },
      "outputs": [],
      "source": [
        "# Below we have implemented a function for every pre processing teqnique with a breif explanation into what it does\n",
        "# This approach was taken to keep our code more organinzed, and maintainable.\n",
        "# Every time we train our model and get our results, we can simply come back to the functions and tweak them as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kdParhcKJ4pk"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filename, sample_frac=0.1, save_sample=False, output_filename='sampled_dataset.csv'):\n",
        "    # Load the entire dataset\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Sample 10% of the data randomly without replacement\n",
        "    sample = data.sample(frac=sample_frac, random_state=42)\n",
        "\n",
        "    # Optionally save the sampled data to a new file\n",
        "    if save_sample:\n",
        "        sample.to_csv(output_filename, index=False)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def avg_trans_amt_per_card(data):\n",
        "    # Average Transaction Amount Per Card: Used to calculate the average amount of money\n",
        "    # spent in transactions for each credit card in the dataset. Once we have the average we\n",
        "    # will use it to compare each transaction amount to the average, and can help identify\n",
        "    # transactions that are too high.\n",
        "    # Group by credit card number and calculate the mean transaction amount\n",
        "    average_amt_per_card = data.groupby('cc_num')['amt'].mean().reset_index()\n",
        "    average_amt_per_card.rename(columns={'amt': 'avg_amt_per_card'}, inplace=True)\n",
        "    # Merge the average transaction amount per card back to both the train and test datasets\n",
        "    data = data.merge(average_amt_per_card, on='cc_num', how='left')\n",
        "    # Create a new column to compare transaction amount to the average per card\n",
        "    data['amt_vs_avg'] = data['amt'] / data['avg_amt_per_card']\n",
        "    return data\n",
        "\n",
        "def trans_freq_per_card(data):\n",
        "    # Transaction Frequency Per Card: Used to count how many transactions are made each\n",
        "    # day per credit card. Helps detect unusual activity if there are more transactions per\n",
        "    # day than the usual pattern.\n",
        "    # Calculate transaction frequency per card per day\n",
        "    trans_freq_per_card = data.groupby(['cc_num', 'trans_date']).size().reset_index(name='trans_freq_per_day')\n",
        "    data = data.merge(trans_freq_per_card, on=['cc_num', 'trans_date'], how='left')\n",
        "    return data\n",
        "\n",
        "def change_in_spending(data):\n",
        "    # Change in Spending Pattern Per Card: Used to compare the current transaction amount to the average\n",
        "    # amount spent for a similar category. By detecting significant deviations in spending patters per\n",
        "    # category we can detect fraud.\n",
        "    # Calculate average spending per card per category\n",
        "    avg_spending_per_card_category = data.groupby(['cc_num', 'category'])['amt'].transform('mean')\n",
        "    data['change_in_spending'] = data['amt'] / avg_spending_per_card_category\n",
        "    return data\n",
        "\n",
        "def handle_date_time(data):\n",
        "    # Converting trans_date_trans_time into a datetime object, then making new cols: trans_date and trans_time\n",
        "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S') # converting the 'trans_date_trans_time' column from a string to a datetime object\n",
        "    data['trans_date'] = data['trans_date_trans_time'].dt.date # extracts the date part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_date'.\n",
        "    data['trans_time'] = data['trans_date_trans_time'].dt.time # extracts the time part from the 'trans_date_trans_time' datetime object and stores it in a new column called 'trans_time'.\n",
        "    return data # returns 2 new cols\n",
        "\n",
        "def dates_since_last_purchase(data):\n",
        "    # Days Since Last Purchase Per Card: Used to calculate the number of days between each transaction\n",
        "    # for the same credit card. Helps detect a pattern of how frequently the card is being used; for example,\n",
        "    # if the card is used 2-3 times a day and all of a sudden the card is being used 10 times a day for\n",
        "    # 2 days straight, it could be fraud.\n",
        "    # Ensure data is sorted by date for correct days calculation\n",
        "    data.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
        "    # Calculate the number of days between each transaction for the same credit card\n",
        "    data['days_since_last'] = data.groupby('cc_num')['trans_date_trans_time'].diff().dt.days.fillna(0).astype(int)\n",
        "    return data\n",
        "\n",
        "def convert_to_numerical_data(data):\n",
        "    # Convert 'trans_date_trans_time' to total seconds elapsed since midnight\n",
        "    data['trans_time_seconds'] = data['trans_date_trans_time'].dt.hour * 3600 + data['trans_date_trans_time'].dt.minute * 60 + data['trans_date_trans_time'].dt.second\n",
        "\n",
        "    # Perform one-hot encoding for 'merchant' and 'category'\n",
        "    data = pd.get_dummies(data, columns=['merchant', 'category'])\n",
        "\n",
        "    # Convert 'trans_date' to datetime and extract features\n",
        "    data['trans_date'] = pd.to_datetime(data['trans_date'])\n",
        "    data['trans_date_year'] = data['trans_date'].dt.year\n",
        "    data['trans_date_month'] = data['trans_date'].dt.month\n",
        "    data['trans_date_day'] = data['trans_date'].dt.day\n",
        "    # Convert 'trans_time' to total seconds elapsed since midnight\n",
        "    data['trans_time_seconds'] = data['trans_time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
        "    # Drop the original non-numeric columns\n",
        "    data.drop(['trans_date_trans_time', 'trans_date', 'trans_time'], axis=1, inplace=True)\n",
        "    return data\n",
        "\n",
        "def drop_unnecessary_cols(data):\n",
        "    # Drop unnecessary columns\n",
        "    cols_to_drop = ['first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'job', 'dob', 'trans_num', 'unix_time']\n",
        "    data = data.drop(cols_to_drop, axis=1)\n",
        "    return data\n",
        "\n",
        "def separate_features_and_labels(data, label_column_name):\n",
        "    # Since 'is_fraud' is the label column, separate the features and labels\n",
        "    X = data.drop(label_column_name, axis=1)  # Drop the label column to create the features set\n",
        "    y = data[label_column_name]              # Get the label column as the labels set\n",
        "    return X, y\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data = handle_date_time(data)\n",
        "    data = avg_trans_amt_per_card(data)\n",
        "    data = trans_freq_per_card(data)\n",
        "    data = dates_since_last_purchase(data)\n",
        "    data = change_in_spending(data)\n",
        "    data = convert_to_numerical_data(data)\n",
        "    data = drop_unnecessary_cols(data)\n",
        "    data = data.dropna(subset=['is_fraud'])  # Drop rows where 'is_fraud' is NaN\n",
        "    # data = split_features(data)\n",
        "    return data\n",
        "\n",
        "def scale_data(X_train, X_val):\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "    # Fit the scaler only on the training data\n",
        "    scaler.fit(X_train)\n",
        "    # Apply the transformation to the training data\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    # Apply the same transformation to the validation data\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    return X_train_scaled, X_val_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUYU6U-OJ4pl"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ LOAD DATA ------------------------------\n",
        "# Load data\n",
        "# ------------------------------ LOAD DATA ------------------------------\n",
        "# Load data\n",
        "train_data = load_dataset('fraudTrain.csv', sample_frac=0.20, save_sample=True, output_filename='sampled_train.csv')\n",
        "test_data = load_dataset('fraudTest.csv', sample_frac=0.20, save_sample=True, output_filename='sampled_test.csv')\n",
        "\n",
        "\n",
        "# ------------------------------ DROP COLS ------------------------------\n",
        "# Remove first column since it is unnecessary\n",
        "train_data = train_data.iloc[:, 1:] # DONE\n",
        "test_data = test_data.iloc[:, 1:] # DONE\n",
        "\n",
        "# --------------------------- PREPROCESS DATA ---------------------------\n",
        "# Preporcess the data\n",
        "train_data = preprocess_data(train_data)\n",
        "test_data = preprocess_data(test_data)\n",
        "\n",
        "# --------------------------- SPLIT FEATURES ---------------------------\n",
        "X_train, y_train = separate_features_and_labels(train_data, 'is_fraud') # This is the only one were spliting into testing and training\n",
        "X_test, y_test = separate_features_and_labels(test_data, 'is_fraud')\n",
        "\n",
        "\n",
        "# -------------------------- PLAN FOR EMPTY VALS -------------------------\n",
        "# Ensure no missing values are left untreated\n",
        "# This method will fill any NaN values in your feature sets with the mean of\n",
        "#their respective columns, which can help in maintaining the integrity of your dataset\n",
        "# without dropping valuable data points.\n",
        "X_train.fillna(X_train.mean(), inplace=True)\n",
        "y_train.fillna(X_train.mean(), inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the training data into training and validation sets:\n",
        "# train_test_split: This function is used to randomly split the\n",
        "# training data into new training data (X_train_final, y_train_final)\n",
        "# and validation data (X_val, y_val), ensuring that the validation\n",
        "# data is representative but not seen during training.\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "# X_train_scaled and X_val_scaled are ready for model training and validation\n",
        "X_train_scaled, X_val_scaled = scale_data(X_train, X_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl2jVSCXm2nS",
        "outputId": "a49cfc21-eb02-4456-f830-390b6825ee05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Naives Bayes Model\n",
        "model = GaussianNB()\n",
        "\n",
        "    # Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "predictions = model.predict(X_val_scaled)\n",
        "\n",
        "    # Evaluate the model\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "conf_matrix = confusion_matrix(y_val, predictions)\n",
        "report = classification_report(y_val, predictions)\n",
        "\n",
        "\n",
        "    # Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "    # Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "    # Additional stats\n",
        "TP, FN, FP, TN = conf_matrix.ravel()  # Extract True Negatives, False Positives, False Negatives, True Positives\n",
        "total_missclassified = FP + FN\n",
        "print(f\"Total Missclassified Transactions: {total_missclassified}\")\n",
        "print(f\"False Positives (Non-Fraud classified as Fraud): {FP}\")\n",
        "print(f\"False Negatives (Fraud classified as Non-Fraud): {FN}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25433dda-80bd-4c81-ac12-c2f83832eb7a",
        "id": "gKhV_9D5vSOF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[2766  623]\n",
            " [  25   10]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.82      0.90      3389\n",
            "         1.0       0.02      0.29      0.03        35\n",
            "\n",
            "    accuracy                           0.81      3424\n",
            "   macro avg       0.50      0.55      0.46      3424\n",
            "weighted avg       0.98      0.81      0.89      3424\n",
            "\n",
            "Total Missclassified Transactions: 648\n",
            "False Positives (Non-Fraud classified as Fraud): 25\n",
            "False Negatives (Fraud classified as Non-Fraud): 623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nXpSSWmJ4pm",
        "outputId": "b2b4c5b9-b47b-4a60-e7bb-20e9318b341c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9883177570093458\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99      3389\n",
            "         1.0       0.37      0.20      0.26        35\n",
            "\n",
            "    accuracy                           0.99      3424\n",
            "   macro avg       0.68      0.60      0.63      3424\n",
            "weighted avg       0.99      0.99      0.99      3424\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ----------------------- LOGISTIC REGRESSION MODEL ----------------------\n",
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000)  # Adjust max_iter if convergence issues occur\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "lr_val_predictions = lr_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_val, lr_val_predictions))\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_val, lr_val_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b5kTzwAJ4pn",
        "outputId": "390fbea3-bec0-4e4a-ec5a-49af9d40342b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with SMOTE Accuracy: 0.9725467289719626\n",
            "Logistic Regression with SMOTE Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.98      0.99      3389\n",
            "         1.0       0.14      0.34      0.20        35\n",
            "\n",
            "    accuracy                           0.97      3424\n",
            "   macro avg       0.57      0.66      0.59      3424\n",
            "weighted avg       0.98      0.97      0.98      3424\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ----------------------- LOGISTIC REGRESSION MODEL ----------------------\n",
        "# ------------------------------ USING SMOTE -----------------------------\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "\n",
        "# Create a pipeline that first oversamples and then runs the logistic regression\n",
        "pipeline = make_pipeline(smote, LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Fit on the training data\n",
        "pipeline.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "lr_val_predictions = pipeline.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression with SMOTE Accuracy:\", accuracy_score(y_val, lr_val_predictions))\n",
        "print(\"Logistic Regression with SMOTE Classification Report:\")\n",
        "print(classification_report(y_val, lr_val_predictions))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}